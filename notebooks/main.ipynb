{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Comparison in The Domain of Brain Tumor Image Classification\n",
    "For our final project for Spring 2025, CS 4644 - Deep Learning, we analyze and compare the results from three unique models:\n",
    "1. 3D CNN - Turning 2D images into 3D datapoints to reconstruct a full brain image.\n",
    "2. ResNet18 - Applying transfer learning by taking a pretrained ResNet-18 model (trained on ImageNet) and adapting it to MRI scans through the fine-tuning of a final fully connected layer.\n",
    "3. Inception - Applying transfer learning in the same way as ResNet, but for another popular and successful model.\n",
    "\n",
    "The following code allows the reader to experiment with these 3 models and observe their results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 0: Get necessary imports, set global variables, and setup dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "# Basic imports\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "import torch\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Model imports\n",
    "from src.models import (\n",
    "    CNN_3D,\n",
    "    MyResNet,\n",
    "    MyInception\n",
    ")\n",
    "# Helper imports\n",
    "from utils.utils import save_trained_model_weights\n",
    "from utils.dataset_utils import prepare_dataset\n",
    "from src import (\n",
    "    Trainer,\n",
    "    get_optimizer\n",
    ")\n",
    "from data.data_transforms import (\n",
    "    get_fundamental_transforms,\n",
    "    get_fundamental_normalization_transforms,\n",
    "    get_fundamental_augmentation_transforms,\n",
    "    get_all_transforms,\n",
    ")\n",
    "from utils.confusion_matrix import (\n",
    "    generate_confusion_data,\n",
    "    generate_confusion_matrix,\n",
    "    plot_confusion_matrix,\n",
    "    get_pred_images_for_target,\n",
    "    generate_and_plot_confusion_matrix,\n",
    "    generate_and_plot_accuracy_table,\n",
    ")\n",
    "\n",
    "# Folder paths\n",
    "raw_data_path = \"../data/raw/\"\n",
    "data_path = \"../data/processed/\"\n",
    "model_path = \"../src/models/\"\n",
    "results_path = \"../results/\"\n",
    "\n",
    "# Global Variables\n",
    "batch_size = 32\n",
    "num_classes = 3 # Output classes: Pituitary, Meningioma, and Glioma Tumor\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Donwload the dataset, preprocess it, and compute statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Kaggle API token found.\n",
      "[INFO] Raw data already exists. Skipping Kaggle download.\n",
      "[INFO] Processed data directory already exists. Skipping conversion.\n",
      "[INFO] Loading cached dataset mean and std...\n"
     ]
    }
   ],
   "source": [
    "dataset_mean, dataset_std = prepare_dataset(\n",
    "    raw_data_path=raw_data_path,\n",
    "    pickle_path= os.path.join(raw_data_path, \"brain_tumor_mri/new_dataset/training_data.pickle\"),\n",
    "    processed_data_path=data_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Test 3D Convolutional Nerual Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_slices = 1 # TODO: Change this if we find a pattern in the dataset for determining multiple images go to same patient\n",
    "inp_size = (n_slices, 512, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flattened size: 11025\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "If size is a sequence, it should have 1 or 2 values",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      3\u001b[39m cnn_optimizer_config = {\u001b[33m\"\u001b[39m\u001b[33moptimizer_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33madam\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mlr\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m1e-3\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mweight_decay\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m1e-8\u001b[39m} \u001b[38;5;66;03m# Tune these\u001b[39;00m\n\u001b[32m      4\u001b[39m cnn_optimizer = get_optimizer(model_cnn, cnn_optimizer_config)\n\u001b[32m      6\u001b[39m cnn_trainer = Trainer(\n\u001b[32m      7\u001b[39m     data_dir=data_path,\n\u001b[32m      8\u001b[39m     model=model_cnn,\n\u001b[32m      9\u001b[39m     optimizer=cnn_optimizer,\n\u001b[32m     10\u001b[39m     model_dir=os.path.join(model_path, \u001b[33m\"\u001b[39m\u001b[33mCNN_3D\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     11\u001b[39m     train_data_transforms=get_all_transforms((inp_size[\u001b[32m1\u001b[39m], inp_size[\u001b[32m2\u001b[39m]), [dataset_mean], [dataset_std]),\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     val_data_transforms=\u001b[43mget_fundamental_normalization_transforms\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m        \u001b[49m\u001b[43minp_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mdataset_mean\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mdataset_std\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m     15\u001b[39m     batch_size=batch_size,\n\u001b[32m     16\u001b[39m     load_from_disk=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     17\u001b[39m     cuda=torch.cuda.is_available(),\n\u001b[32m     18\u001b[39m     n_slices=n_slices,\n\u001b[32m     19\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Downloads/project-folder/Git/TumorTrace/TumorTrace/data/data_transforms.py:36\u001b[39m, in \u001b[36mget_fundamental_normalization_transforms\u001b[39m\u001b[34m(inp_size, pixel_mean, pixel_std)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_fundamental_normalization_transforms\u001b[39m(\n\u001b[32m     33\u001b[39m     inp_size: Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m], pixel_mean: Sequence[\u001b[38;5;28mfloat\u001b[39m], pixel_std: Sequence[\u001b[38;5;28mfloat\u001b[39m]\n\u001b[32m     34\u001b[39m ) -> transforms.Compose:\n\u001b[32m     35\u001b[39m     fund_norm_transforms = transforms.Compose([\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m         \u001b[43mtransforms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mResize\u001b[49m\u001b[43m(\u001b[49m\u001b[43minp_size\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m     37\u001b[39m         transforms.ToTensor(),\n\u001b[32m     38\u001b[39m         transforms.Normalize(mean=pixel_mean, std=pixel_std),\n\u001b[32m     39\u001b[39m     ])\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fund_norm_transforms\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/Caskroom/miniconda/base/envs/TumorTrace/lib/python3.11/site-packages/torchvision/transforms/transforms.py:343\u001b[39m, in \u001b[36mResize.__init__\u001b[39m\u001b[34m(self, size, interpolation, max_size, antialias)\u001b[39m\n\u001b[32m    341\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSize should be int or sequence. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(size)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    342\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(size, Sequence) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m343\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mIf size is a sequence, it should have 1 or 2 values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    344\u001b[39m \u001b[38;5;28mself\u001b[39m.size = size\n\u001b[32m    345\u001b[39m \u001b[38;5;28mself\u001b[39m.max_size = max_size\n",
      "\u001b[31mValueError\u001b[39m: If size is a sequence, it should have 1 or 2 values"
     ]
    }
   ],
   "source": [
    "model_cnn = CNN_3D(inp_size, num_classes=num_classes)\n",
    "inp_size = (512, 512)\n",
    "cnn_optimizer_config = {\"optimizer_type\": \"adam\", \"lr\": 1e-3, \"weight_decay\": 1e-8} # Tune these\n",
    "cnn_optimizer = get_optimizer(model_cnn, cnn_optimizer_config)\n",
    "\n",
    "cnn_trainer = Trainer(\n",
    "    data_dir=data_path,\n",
    "    model=model_cnn,\n",
    "    optimizer=cnn_optimizer,\n",
    "    model_dir=os.path.join(model_path, \"CNN_3D\"),\n",
    "    train_data_transforms=get_all_transforms(inp_size, [dataset_mean], [dataset_std]),\n",
    "    val_data_transforms=get_fundamental_normalization_transforms(\n",
    "        inp_size, [dataset_mean], [dataset_std]\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    load_from_disk=False,\n",
    "    cuda=torch.cuda.is_available(),\n",
    "    n_slices=n_slices,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cnn_trainer.run_training_loop(num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_trainer.plot_loss_history()\n",
    "cnn_trainer.plot_accuracy()\n",
    "\n",
    "cnn_train_accuracy = cnn_trainer.train_accuracy_history[-1]\n",
    "cnn_validation_accuracy = cnn_trainer.validation_accuracy_history[-1]\n",
    "print(\n",
    "    \"Train Accuracy = {}; Validation Accuracy = {}\".format(\n",
    "        cnn_train_accuracy, cnn_validation_accuracy\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_trained_model_weights(model_cnn, out_dir=os.path.join(model_path, \"CNN_3D\"))\n",
    "cnn_trainer.save_plots(path=results_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Test ResNet Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_size = (224,224) # Double check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_resnet = MyResNet(num_classes=num_classes)\n",
    "\n",
    "resnet_optimizer_config = {\"optimizer_type\": \"adam\", \"lr\": 1e-3, \"weight_decay\": 1e-8} # Tune these\n",
    "resnet_optimizer = get_optimizer(model_resnet, resnet_optimizer_config)\n",
    "\n",
    "resnet_trainer = Trainer(\n",
    "    data_dir=data_path,\n",
    "    model=model_resnet,\n",
    "    optimizer=resnet_optimizer,\n",
    "    model_dir=os.path.join(model_path, \"ResNet\"),\n",
    "    train_data_transforms=get_all_transforms(inp_size, [dataset_mean], [dataset_std]),\n",
    "    val_data_transforms=get_fundamental_normalization_transforms(\n",
    "        inp_size, [dataset_mean], [dataset_std]\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    load_from_disk=False,\n",
    "    cuda=torch.cuda.is_available(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "resnet_trainer.run_training_loop(num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_trainer.plot_loss_history()\n",
    "resnet_trainer.plot_accuracy()\n",
    "\n",
    "resnet_train_accuracy = resnet_trainer.train_accuracy_history[-1]\n",
    "resnet_validation_accuracy = resnet_trainer.validation_accuracy_history[-1]\n",
    "print(\n",
    "    \"Train Accuracy = {}; Validation Accuracy = {}\".format(\n",
    "        resnet_train_accuracy, resnet_validation_accuracy\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_trained_model_weights(model_resnet, out_dir=os.path.join(model_path, \"ResNet\"))\n",
    "resnet_trainer.save_plots(path=results_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Test Inception Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_size = (299,299) # Double check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inception = MyInception(num_classes=num_classes)\n",
    "\n",
    "inception_optimizer_config = {\"optimizer_type\": \"adam\", \"lr\": 1e-3, \"weight_decay\": 1e-8} # Tune these\n",
    "inception_optimizer = get_optimizer(model_inception, inception_optimizer_config)\n",
    "\n",
    "inception_trainer = Trainer(\n",
    "    data_dir=data_path,\n",
    "    model=model_inception,\n",
    "    optimizer=inception_optimizer,\n",
    "    model_dir=os.path.join(model_path, \"Inception\"),\n",
    "    train_data_transforms=get_all_transforms(inp_size, [dataset_mean], [dataset_std]),\n",
    "    val_data_transforms=get_fundamental_normalization_transforms(\n",
    "        inp_size, [dataset_mean], [dataset_std]\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    load_from_disk=False,\n",
    "    cuda=torch.cuda.is_available(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "inception_trainer.run_training_loop(num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inception_trainer.plot_loss_history()\n",
    "inception_trainer.plot_accuracy()\n",
    "\n",
    "inception_train_accuracy = inception_trainer.train_accuracy_history[-1]\n",
    "inception_validation_accuracy = inception_trainer.validation_accuracy_history[-1]\n",
    "print(\n",
    "    \"Train Accuracy = {}; Validation Accuracy = {}\".format(\n",
    "        inception_train_accuracy, inception_validation_accuracy\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_trained_model_weights(model_inception, out_dir=os.path.join(model_path, \"Inception\"))\n",
    "inception_trainer.save_plots(path=results_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Analyze Graphs and Final Accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('3D CNN:', model_cnn.count_parameters())\n",
    "print('ResNet:', model_resnet.count_parameters())\n",
    "print('Inception:', model_inception.count_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss & Accuracy Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_trainer.plot_loss_history()\n",
    "cnn_trainer.plot_accuracy()\n",
    "\n",
    "cnn_train_accuracy = cnn_trainer.train_accuracy_history[-1]\n",
    "cnn_validation_accuracy = cnn_trainer.validation_accuracy_history[-1]\n",
    "print(\n",
    "    \"Train Accuracy = {}; Validation Accuracy = {}\".format(\n",
    "        cnn_train_accuracy, cnn_validation_accuracy\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_trainer.plot_loss_history()\n",
    "resnet_trainer.plot_accuracy()\n",
    "\n",
    "resnet_train_accuracy = resnet_trainer.train_accuracy_history[-1]\n",
    "resnet_validation_accuracy = resnet_trainer.validation_accuracy_history[-1]\n",
    "print(\n",
    "    \"Train Accuracy = {}; Validation Accuracy = {}\".format(\n",
    "        resnet_train_accuracy, resnet_validation_accuracy\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inception_trainer.plot_loss_history()\n",
    "inception_trainer.plot_accuracy()\n",
    "\n",
    "inception_train_accuracy = inception_trainer.train_accuracy_history[-1]\n",
    "inception_validation_accuracy = inception_trainer.validation_accuracy_history[-1]\n",
    "print(\n",
    "    \"Train Accuracy = {}; Validation Accuracy = {}\".format(\n",
    "        inception_train_accuracy, inception_validation_accuracy\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_and_plot_confusion_matrix(model_cnn, cnn_trainer.val_dataset, path=results_path, use_cuda=torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_and_plot_confusion_matrix(model_resnet, resnet_trainer.val_dataset, path=results_path, use_cuda=torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_and_plot_confusion_matrix(model_inception, inception_trainer.val_dataset, path=results_path, use_cuda=torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze errors that occurred from confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = resnet_trainer # Change this\n",
    "model = model_resnet # Change this\n",
    "\n",
    "# Analyze confusion matrix and change these to observe results\n",
    "predicted_class_num = 0\n",
    "true_class_num = 0\n",
    "\n",
    "correct_class = [k for k, v in trainer.val_dataset.class_dict.items() if v == true_class_num][0]\n",
    "pred_class = key = [k for k, v in trainer.val_dataset.class_dict.items() if v == predicted_class_num][0]\n",
    "print(trainer.val_dataset.class_dict)\n",
    "\n",
    "paths = get_pred_images_for_target(model, trainer.val_dataset, predicted_class_num, true_class_num, torch.cuda.is_available())\n",
    "max_count = 10\n",
    "count = 0\n",
    "for path in paths:\n",
    "    img = Image.open(path).convert(mode='L')\n",
    "    if (count != max_count):\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.title(f'Image of {correct_class}, misclassified as {pred_class}')\n",
    "        plt.axis('off')  # Removes axis ticks\n",
    "        plt.show()\n",
    "        count += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TumorTrace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
